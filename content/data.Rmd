---
title: Data
description:
toc: true
featuredVideo:
featuredImage: images/main_pages/data_cloud.jpg
draft: false
---

```{r setup, include = F}

knitr::opts_chunk$set(eval = F, include = T)

```

## Overview

Because no data source on its own could really tell a coherent story about the trends in spending and population for species in North America, we use multiple main data sources are used throughout this project.  The four main pieces of data used are:<br><br>

* Living Planet Index (LPI) source data
  * File Name: `LPR2020data_public.csv`
  * Source: [http://stats.livingplanetindex.org/](http://stats.livingplanetindex.org/)
  * Alias(es) in Code: `lpi`<br><br>
  
* IUCN Red List
  * File Name: `redlistspecies.csv`
  * Source: [https://www.iucnredlist.org/statistics](https://www.iucnredlist.org/statistics)
  * Alias(es) in Code: `redlist`, `red`<br><br>
  
* US Fish and Wildlife Service (FWS) expenditure reports
  * Directory Name: `expen`
  * Source: [https://www.fws.gov/endangered/esa-library/](https://www.fws.gov/endangered/esa-library/)
  * Alias(es) in Code: `exp`, `exp_<X>` (where \<X\> is the expenditure year)<br><br>

* US FWS recovery plan reports
  * File Names: `FWS_full.csv`, `FWS_recovery.csv`
  * Source: [https://ecos.fws.gov/ecp/report/species](https://ecos.fws.gov/ecp/report/species)
  * Aliases in Code: `fws`, `fws_data`, `fws_recovery`<br><br>

The full data loading and cleaning process can be found [here](/load_and_clean_data.R).  Running the file requires each of the datasets mentioned above, all of which can be found [here](https://drive.google.com/drive/folders/1o5BlVfD8VJT2DUQfLKJjHZeO-8vIXPcG?usp=sharing).  The linked `load_and_clean_data.R` file also writes csv files with the major cleaned data to a directory called `clean` within the `dataset` directory.<br><br>

Other than `tidyverse` and `readxl`, which are used to read, sort, and "tidy" the data, the package `janitor` is used to quickly and consistently clean variable names.<br><br>

**Quick Links:**<br>
[Living Planet Index](#living-planet-index): [Variables](#variables), [Cleaning](#cleaning)

[IUCN Red List](#iucn-red-list): [Variables](#variables-1), [Cleaning](#cleaning-1)

[FWS Expenditures](#fws-expenditures): [Variables](#variables-2), [Cleaning](#cleaning-2)

[FWS Recovery Plans](#fws-recovery-plans): [Variables](#variables-3), [Cleaning](#cleaning-3)

[Joining Data](#joining-data)

## Living Planet Index

The first dataset used was sourced from the [Living Planet Index](http://stats.livingplanetindex.org/), and lists population counts from 1950 to 2018 for a large range of species. According to the LPI website, the data is “currently collated from more than 3,000 individual data sources such as published scientific literature, online databases and grey literature,” for the purpose of calculating the "Living Planet Index", a measure of the health of the biosphere as a whole.  Since the site requires registration, the data can also be found [here](https://drive.google.com/drive/folders/1fxfnjhI6KyCK25uLY8px48zu1GR2N2H_?usp=sharing).

### Variables

A large number of features are provided as part of the LPI dataset.  However, only a few proved useful.  This is largely because of the large variety of units used for the counts and the disparity in data availability per location.<br><br>

Notable variables (in `lpi_data`):<br><br>

* `id`: unique identification for each species' data, per location.
* `binomial:common_name`: taxonomic information for each species; `binomial` is used as a unique identifier per species.
* `location:m_biome`: location data; mostly unused.
* `units` and `method`: the units of the counts given; some of the most frequent units can be seen in the word cloud featured on this page.
* `x1950:x2018`: count data.

### Cleaning

Once it was decided that the units could not effectively be reconciled, the cleaning for the LPI dataset was fairly simple.  As below, the data is fairly simply imported, notably ensuring that `id` is treated as a factor, and two features which are certainly not useful are removed.  Then, the data is pivoted into a time series, which makes it much easier to use `year` as a predictor. Additionally, `NA` values are removed.<br><br>

```{r lpi}

# import
lpi_data <- read_csv(here::here('dataset', 'LPR2020data_public.csv'),
                     na = 'NULL',
                     col_types = paste('fccccccccccccccddcccccccccccc',
                                       paste(rep('d', 69), collapse = ''),
                                       sep = '')) %>% 
   clean_names() %>% 
   select(-c('reference', 'citation')) 

# convert to time series
lpi_ts <- lpi_data %>% 
  pivot_longer(cols = x1950:x2018,
               names_to = 'year',
               values_to = 'count') %>% 
   mutate(year = as.integer(gsub('x', '', year))) %>% 
   filter(!is.na(count))

```

## IUCN Red List

The second dataset is sourced from the IUCN (International Union for Conservation of Nature).  It can be found [here](https://www.iucnredlist.org/statistics).  The data contains information on numbers of species per endangerment status in different taxonomic classes, in different locations, and over time---this data is used to compare global endangerment statuses of different species.  Species extinction risk assessment is conducted by members of the IUCN Species Survival Commission (SSC), appointed Red List Authorities (RLAs), Red List Partners, or specialists working on IUCN-led assessment projects according to IUCN Red List Categories and Criteria.

### Variables

Like the LPI dataset, the redlist dataset contains a significant amount of data on the taxonomy of each species.  The redlist data additionally provides redlist category (which essentially amounts to endangerment status) and population trends.<br><br>

Notable variables (in `redlist_data`):<br><br>

* `scientific_name:species_name`: taxonomic information for each species.
* `redlist_category`: self-explanatory. Endangerment status.
* `population_trend`: exactly what it sounds like---population trends.

### Cleaning

Also like the LPI data, the redlist data can be imported with little specific cleaning.  Certainly unuseful features are removed, and the scientific names are converted into "binomial" form (`Genus_species`), to match the other datasets.<br><br>

```{r redlist}

redlist_data <- read_csv(here::here('dataset', 'redlistspecies.csv'),
                         col_types = 'ffccccccccccccfccff') %>% 
   clean_names() %>% 
   select(-c('internal_taxon_id',
             'infra_type',
             'infra_name',
             'infra_authority',
             'redlist_criteria')) %>% 
   mutate(binomial = gsub(' ', '_', scientific_name))

```

## FWS Expenditures

The third major dataset used is an amalgamation of many datasets provided per year by the United States Fish and Wildlife Service (FWS).  These datasets can be found [here](https://www.fws.gov/endangered/esa-library/).  FWS Expenditures are noted by species and sometimes subspecies in various locations, and are available from 2000-2017.  The keeping of these records is mandated by the Endangered Species Act (ESA) of 1973, which requires "an accounting on a species by species basis of all reasonably identifiable Federal expenditures" for the purpose of cost analysis.

### Variables

Each year of expenditures comes in its own Excel file, each of which has a slightly different structure and format (consistency is apparently not a trait of government datasets).  In general, each of these yearly datasets has the following notable features, though some are missing or formatted differently.<br><br>

Notable variables (in `raw_exp_<X>`):<br><br>

* `Class`: class of each species.
* `Species (50 CFR Part 17)`: species name and taxonomic information; sometimes also location.
* `Status`: endangerment status as defined by the FWS.
* `FWS Total ($):Species Total ($)`: spending per year per species, from different sources and/or in different categories.

### Cleaning

Evidenced by the multitude of diverse Excel files, cleaning is a major endeavor for the expenditure files.  Each year, the FWS formats its expenditure reports differently, with different methods of recording species names, locations--- sometimes with species name and location housed in the same variable---and various document layouts. Therefore unique code is required to read and clean each year. The PDF files are first converted to Excel files, which are then imported into dataframes named `raw_exp_<X>`, where `<X>` is the data year.  As shown below, sheet must also be specified, and differs year to year.<br><br>

```{r expenditure-import}

# 2000
raw_exp_2000 <- read_excel(here::here('dataset',
                                      'expen/expenditure_2000.xlsx'),
                           # specify sheet number
                           sheet = 3)

```

<br>Because many of the files have a significant amount of data in a single column, a few functions were created to sort out bracketing and extract the relevant data.  The `open_br` function extracts characters from within a final pair of parentheses in a string, the `rm_br` function deletes characters within a final pair of parentheses in a string, and `read_br` combines these two functions to recursively remove content in parentheses until the scientific name can be revealed.  Though there are odd formatting challenges in the data which these functions, on occasion, miss, the majority of the data points' scientific names are properly recovered using these functions.<br><br>

```{r expenditure-functions}

# function to read outermost bracket
# example: 'welcome (home) (my(lovely) gator)' %>% last_br()
# returns: 'my(lovely) gator'
last_br <- function(string) {
  
  # initializing counters
  open_br <- 0
  start_pos <- 0
  end_pos <- 0
  
  # list of string characters
  string_list <- unlist(strsplit(string, split = ""))
  
  # finds final outermost opening and closing brackets
  for (i in 1:nchar(string)) {
    if (string_list[i] == '(') {
      if (open_br == 0) {
        start_pos = i
      }
      open_br <- open_br + 1
    } else if (string_list[i] == ')') {
      if (open_br == 1) {
        end_pos = i
      }
      open_br <- open_br - 1
    }
  }
  
  # finding and returning string in last bracket
  string <- substr(string, start_pos + 1, end_pos - 1)
  return(string)
}

# function to remove final brackets and content within
# example: 'my(lovely) gator' %>% rm_br()
# returns 'my gator'
rm_br <- function (string) {
  # ignore empty strings
  if (nchar(string) == 0) {return(string)}
  
  # initializing counters
  open_br <- 0
  start_pos <- 0
  end_pos <- 0
  
  # list of string characters
  string_list <- unlist(strsplit(string, split = ""))
  
  # finds start and end brackets
  for (i in 1:nchar(string)) {
    if (string_list[i] == '(') {
      if (open_br == 0) {
        start_pos = i
      }
      open_br <- open_br + 1
    } else if (string_list[i] == ')') {
      if (open_br == 1) {
        end_pos = i
      }
      open_br <- open_br - 1
    }
  }
  
  # finding and returning string except last bracket
  string <- paste(substr(string, 1, start_pos - 1),
                  substr(string, end_pos + 1, nchar(string)),
                  sep = '')
  return(string)
}

# combining functions to read last bracket without inner
# example: 'welcome (home) (my(lovely) gator)' %>% read_br()
# returns: 'my gator'
# ignores if bracket begins with '=', is too short, begins with lower
# returns NA if blank
read_br <- function (string) {
  
  if (!all(grepl('\\(', string),
           str_count(string, '\\)') == str_count(string, '\\('))) {
     return(string)
  }
  
  bracket <- rm_br(last_br(string))
  
  # avoiding too-small or false brackets
  if (any(nchar(bracket) <= 2,
          str_detect(substr(bracket, 1, 1), '[[:lower:]]'),
          substr(bracket, 1, 1) == '=')) {
     bracket <- read_br(gsub('  ', ' ', rm_br(string)))
  }
  
  final <- ifelse(bracket == '',
                  NA,
                  bracket)
  
  return(final)
}

```

<br>An example of the cleaning process for a single year (2008):

1. Import the data from sheet 2 of the relevant Excel file.
2. Remove columns beginning with `...`
3. Clean and rename variables.
4. Filter out `NA` values for name and `class`
5. Generate a `binomial` ('Genus_species') representation of the species scientific name, using the above functions.
6. Attach the feature `year` with value 2008 for all data points.<br><br>

```{r expenditure-example}

# 2008
raw_exp_2008 <- read_excel(here::here('dataset',
                                      'expen/expenditure_2008.xlsx'),
                           sheet = 2) %>% 
  select(-starts_with('...'))

exp_2008 <- raw_exp_2008 %>% 
  clean_names() %>% 
  rename(common = species_50_cfr_part_17,
         class = group_name) %>% 
  filter(!is.na(common),
         !is.na(class)) %>% 
  
  # changing scientific to Genus_species, subspecies, genus format
  rowwise() %>% 
  mutate(scientific = read_br(gsub('\r?\n|\r', ' ', common))) %>%
  mutate(scientific = gsub('  ', ' ', scientific)) %>% 
  mutate(binomial = paste(unlist(strsplit(scientific,
                                          split = ' '))[1:2],
                          collapse = '_'),
         subspecies = ifelse(str_count(scientific, ' ') == 2,
                             unlist(strsplit(scientific,
                                             split = ' '))[3],
                             NA),
         genus = paste(unlist(strsplit(scientific,
                                       split = ' '))[1],
                       collapse = '_')) %>% 
  ungroup() %>% 
  mutate(year = 2008)

```

<br>This process is notably complex, and accounts for a significant amount of the time spent on this project.  Though it is not shown here in the interest of space, it should be noted the data from 2011 is particularly difficult to clean, and requires specific grouping and concatenation, beyond that used to clean the other datasets.

## FWS Recovery Plans

The final major piece of data, the FWS species recovery plans, can be found [here](https://ecos.fws.gov/ecp/report/species).  This data notably contains information about conservation plans for each species carried out by the FWS, including plan type,  start time, and recovery stage.  Archives are kept by the US government and FWS to keep track of recovery and spending plan types for each species under protection by the Endangered Species Act (ESA).

### Variables

Moving back to data with reasonable naming convention and generally simpler cleaning, the FWS recovery plan data has a few notable features, in addition to taxonomic and location data.<br><br>

Notable variables (in `fws_<data>`):<br><br>

* `common_name:scientific_name_url`: taxonomic information for each species.
* `esa_listing_status`: species listing under the Endangered Species Act.
* `esa_listing_date`: date of species inclusion under the ESA
* `conservation_plan_type`: what it says on the tin.
* `recovery_document_date`: date of creation of recovery plan
* `recovery_document_stage`: stage of plan (e.g. draft, revision, final)

### Cleaning

Luckily, this data is fairly simple to import and clean, and the functions used to clean the expenditure data can be repurposed. Species identification in the `binomial` form is acquired using a similar process to that used for the expenditure data; otherwise the data is fairly simply imported.  Only the uniquely useful features are maintained from this dataset.<br><br>

```{r recovery}

# recovery plan data
fws_recovery <- read_csv(here::here('dataset', 'FWS_recovery.csv'),
                         col_types = 'cccccccccccc') %>%
  clean_names() %>% 
  rowwise() %>%
  mutate(scientific = read_br(species_scientific_name)) %>% 
  mutate(binomial = paste(unlist(strsplit(scientific,
                                          split = ' '))[1:2],
                          collapse = '_'),
         subspecies = ifelse(str_count(scientific, ' ') == 2,
                             unlist(strsplit(scientific,
                                             split = ' '))[3],
                             NA)) %>% 
  ungroup() %>% 
  select(scientific:subspecies,
         recovery_document_title,
         recovery_document_date:region_name)

```

## Joining Data

Though a few other small datasets (which can be found in the `additional` directory [here](https://drive.google.com/drive/folders/1o5BlVfD8VJT2DUQfLKJjHZeO-8vIXPcG?usp=sharing)) are used for some minor parts of the analysis, these four datasets and the datasets created by joining them are the main focus of the project.<br><br>

The cleaning done on each dataset makes it much simpler to pull each dataset into a combined time series.  Notably, the expenditure data must be concatenated into a single `exp_ts` time series.<br><br>

```{r join-exp}

# all clean expenditures in a list
exp_list <- list(exp_2000,
                 exp_2001,
                 exp_2002,
                 exp_2003,
                 exp_2004,
                 exp_2005,
                 exp_2006,
                 exp_2007,
                 exp_2008,
                 exp_2009,
                 exp_2010,
                 exp_2011,
                 exp_2012,
                 exp_2013,
                 exp_2014,
                 exp_2015,
                 exp_2016,
                 exp_2017) %>%
  lapply(exp_select)

# vertical join to convert to time series
exp_ts <- map_df(exp_list, rbind)

```

<br>Then, the rest of the data can be joined together into a much more useful dataset than any of the datasets alone.  The `lpi_red_ts` data was joined early on, so it is already joined in this example, but the `full_ts` shown below incorporates each of the above mentioned datasets.  A small amount of cleaning was necessary to account for varying class names within the datasets.<br><br>

```{r join-full}

# all datasets merged together
full_ts <- left_join(exp_ts,
                     fws_data,
                     by = 'binomial') %>%
  distinct(.keep_all = TRUE) %>%
  filter(!is.na(scientific_name)) %>%
  left_join(fws_recovery, by = 'binomial')

# small fix to maintain consistency
full_ts$class <- gsub('Amphibias', 'amphibia', full_ts$class)

```

<br>The joined time series data are saved  as csv files in a `clean` directory within the `dataset` directory for quick access, but the `load_and_clean_data.R` file also runs fairly quickly, so it's not unreasonable to run it and rely on the produced environmental variables to carry out the analyses shown on the other pages.  Happy exploring!